{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning a Caffe Model in Tensorflow #\n",
    "\n",
    "https://kratzert.github.io/2017/02/24/finetuning-alexnet-with-tensorflow.html\n",
    "\n",
    "We'll use a pre-trained model on the Oxford-102 flower dataset and fine tune it for the Oxford-17 flower dataset. In the process, we'll have to convert the original Caffe model network into a Tensorflow model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data ##\n",
    "Download the data and Caffe models we'll be using."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```shell\n",
    "$ python bootstrap.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing of Images ##\n",
    "\n",
    "Let's batch process the images by resizing and cropping them to the correct size before inputting them into the network. This will produce a new set of images, which we place in a separate /resized directory. This was a helpful [link](http://www.coderholic.com/batch-image-processing-with-python/) on using the PIL library to do this.\n",
    "\n",
    "```shell\n",
    "$ python batch_process.py ./data/jpg/*.jpg\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Update ##\n",
    "\n",
    "Then we need to convert our old Caffe network files into newer ones so that we can do the Caffe to Tensorflow model conversion. We'll use the upgrade_net_proto_txt and upgrade_net_proto_binary functions that ship with Caffe. We'll use the following commands:\n",
    "\n",
    "```shell\n",
    "$ (CAFFE_ROOT)/build/tools/upgrade_net_proto_text ./caffe/deploy.prototxt ./caffe/deploy_new.prototxt\n",
    "$ (CAFFE_ROOT)/build/tools/upgrade_net_proto_binary ./caffe/oxford102.caffemodel ./caffe/oxford102_new.caffemodel\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Conversion ##\n",
    "\n",
    "Then we'll port our Caffe model to tensorflow using the [caffe-tensorflow](https://github.com/ethereon/caffe-tensorflow) tool. We run the following command:\n",
    "\n",
    "```shell\n",
    "$ (CAFFE_TENSORFLOW_ROOT)/convert.py  --caffemodel ./data/oxford102_new.caffemodel --data-output-path ./data/oxford102.npy ./data/deploy_new.prototxt\n",
    "```\n",
    "\n",
    "We'll only be using the weights (oxford102.npy) from this conversion, as I couldn't get the oxford102.py network to work with my version of Tensorflow. Fortunately for us, there is a readily available Tensorflow port of the AlexNet architecture that seems to work well. You can find the file [here](http://www.cs.toronto.edu/~guerzhoy/tf_alexnet/myalexnet_forward_newtf.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow ##\n",
    "\n",
    "Ok, now the hard part is behind us- let's start using our model in Tensorflow. We'll use the pre-defined Tensorflow network structure for AlexNet, and then just plug our weights in from the Oxford102 network. Let's first convert this file into a Python class so that we get a TensorFlow Network when we create an object of this class. We'll also add some dropout layers after fc6 and fc7 which are not part of the original deploy.prototxt model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load oxfordnet.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write some code to fine-tune our model network now. First, we need to create an image pipeline for loading the data in batches. This [link](http://ischlag.github.io/2016/06/19/tensorflow-input-pipeline-example/) was helpful by using a Tensorflow Queue (this is now outdated and the Datasets approach is preferred)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load prepare_data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the training now by setting up our model and running it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting fine_tune.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile fine_tune.py\n",
    "#!/usr/bin/env python\n",
    "# Trains and tests the model and outputs the mean accuracy results to a txt file\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from datetime import datetime\n",
    "from oxfordnet import Oxford17Net\n",
    "from prepare_data import return_data_splits, read_images_from_disk\n",
    "\n",
    "def train(split_num):\n",
    "    \"\"\"\n",
    "    Function for fine-tuning the Oxford17 dataset on training data.\n",
    "    \n",
    "    Attributes:\n",
    "        split_num: which split to test on, either 1, 2, or 3\n",
    "    \"\"\"\n",
    "        \n",
    "    # Learning params\n",
    "    num_epochs = 10\n",
    "    batch_size = 68\n",
    "\n",
    "    # Network params\n",
    "    dropout_rate = 0.5\n",
    "    num_classes = 17\n",
    "    # We'll just fine tune the final FC layer, but we could also use 'fc6' and 'fc7' here\n",
    "    train_layers = ['fc8'] \n",
    "\n",
    "    # Path to data splits\n",
    "    data_path = './data/'\n",
    "    # Train on a split of the data\n",
    "    train_paths, train_labels = return_data_splits(data_path, split='trn', split_num=split_num)\n",
    "\n",
    "    # Get the number of trainingsteps per epoch\n",
    "    train_batches_per_epoch = np.floor(len(train_paths) / batch_size).astype(np.int16) # 10\n",
    "\n",
    "    # Convert to Tensorflow data types\n",
    "    train_image_paths = tf.convert_to_tensor(train_paths, dtype=tf.string)\n",
    "    train_image_labels = tf.convert_to_tensor(train_labels, dtype=tf.int32)\n",
    "\n",
    "    # create input queues\n",
    "    train_input_queue = tf.train.slice_input_producer(\n",
    "                                        [train_image_paths, train_image_labels],\n",
    "                                        shuffle=True)\n",
    "\n",
    "    # Load images from disk\n",
    "    X_train, Y_train = read_images_from_disk(train_input_queue)\n",
    "\n",
    "    # collect batches of images before processing\n",
    "    x, y = tf.train.batch(\n",
    "                                        [X_train, Y_train],\n",
    "                                        batch_size=batch_size\n",
    "                                        #,num_threads=1\n",
    "                                        )\n",
    "\n",
    "    # Placeholder for dropout probability and learning rate\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    learning_rate = tf.placeholder(tf.float32)\n",
    "\n",
    "    # Load the network weights here for use in the model\n",
    "    net_data = np.load(open(\"./caffe/oxford102.npy\", \"rb\"), encoding=\"latin1\").item()\n",
    "\n",
    "    # Initialize model\n",
    "    model = Oxford17Net(x, num_classes, keep_prob, net_data)\n",
    "\n",
    "    # Link variable to model output\n",
    "    score = model.fc8\n",
    "\n",
    "    # Cross-entropy loss\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = score, labels = y))\n",
    "\n",
    "    # Find the trainable weights and biases here    \n",
    "    tvars = []\n",
    "    for layer in train_layers:\n",
    "        tvars.append([var for var in tf.trainable_variables() if layer in var.name])\n",
    "    trainer = tf.train.AdamOptimizer(learning_rate).minimize(loss, var_list=tvars)\n",
    "\n",
    "    # Accuracy of the model\n",
    "    correct_pred = tf.equal(tf.argmax(score, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "    # Save checkpoints\n",
    "    checkpoint_path = \"./checkpoints/\"\n",
    "    if not os.path.isdir(checkpoint_path): os.mkdir(checkpoint_path)\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    # Now train our model\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        # initialize the variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # initialize the queue threads to start to shovel data\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "        # (optional) learning rate decay\n",
    "        max_learning_rate = 0.001\n",
    "        min_learning_rate = 0.0001\n",
    "        decay_speed = 200.0\n",
    "        \n",
    "        # Loop over number of epochs\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "              print(\"{} Epoch number: {}\".format(datetime.now(), epoch+1))\n",
    "\n",
    "              step = 1                \n",
    "                \n",
    "              while step <= train_batches_per_epoch:\n",
    "            \n",
    "                  lr = max_learning_rate\n",
    "                  # uncomment below if using learning rate decay\n",
    "                  # lr = min_learning_rate + (max_learning_rate - min_learning_rate) * math.exp(-(epoch*train_batches_per_epoch+step-1)/decay_speed)\n",
    "                    \n",
    "                  # Update the weights using backprop\n",
    "                  sess.run(trainer, feed_dict={keep_prob: dropout_rate, learning_rate: lr})\n",
    "\n",
    "                  # Check model performance on the batch\n",
    "                  #l, a = sess.run([loss, accuracy], feed_dict={keep_prob: 1.})\n",
    "                  l, a = sess.run([loss, accuracy], feed_dict={keep_prob: 1., learning_rate: lr})\n",
    "                        \n",
    "                  #print(\"Loss: %f,     Accuracy: %f\" % (l, a))\n",
    "                  print(\"Learning Rate: %f,     Loss: %f,     Accuracy: %f\" % (lr, l, a))\n",
    "\n",
    "                  step += 1\n",
    "\n",
    "        # Save model checkpoint\n",
    "        checkpoint_name = os.path.join(checkpoint_path, 'model' \\\n",
    "                                       + '_split_' + str(split_num) + '.ckpt')\n",
    "        save_path = saver.save(sess, checkpoint_name)  \n",
    "\n",
    "        print(\"{} Model checkpoint saved at {}\".format(datetime.now(), checkpoint_name))\n",
    "\n",
    "        # stop our queue threads and properly close the session\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "        sess.close()\n",
    "\n",
    "def test(split_num, split='tst'):\n",
    "    \"\"\"\n",
    "    Function for testing the fine-tuned Oxford17 dataset on either the validation or test set.\n",
    "    \n",
    "    Args:\n",
    "        split_num: which split to test on, either 1, 2, or 3\n",
    "        split: which split to test, either 'val' for validation or 'tst' for testing\n",
    "    Returns:\n",
    "        Saves a model checkpoint in the /checkpoints directory which can be re-loaded\n",
    "        to evaluate the performance of the model\n",
    "    \"\"\"\n",
    "\n",
    "    # Network params\n",
    "    num_classes = 17\n",
    "    keep_prob = 1.\n",
    "\n",
    "    # Path to data splits\n",
    "    data_path = './data/'\n",
    "    # Use split 1 to test\n",
    "    paths, labels = return_data_splits(data_path, split=split, split_num=split_num)\n",
    "    \n",
    "    # Convert to Tensorflow data types\n",
    "    image_paths = tf.convert_to_tensor(paths, dtype=tf.string)\n",
    "    image_labels = tf.convert_to_tensor(labels, dtype=tf.int32)\n",
    "    batch_size = len(paths)\n",
    "\n",
    "    # create input queues\n",
    "    input_queue = tf.train.slice_input_producer(\n",
    "                                        [image_paths, image_labels],\n",
    "                                        shuffle=False)\n",
    "\n",
    "    # Load images from disk\n",
    "    X, Y = read_images_from_disk(input_queue)\n",
    "\n",
    "    # collect batches of images before processing\n",
    "    x, y = tf.train.batch(\n",
    "                                        [X, Y],\n",
    "                                        batch_size=batch_size\n",
    "                                        #,num_threads=1\n",
    "                                        )\n",
    "\n",
    "    # Load the network weights here for use in the model\n",
    "    net_data = np.load(open(\"./caffe/oxford102.npy\", \"rb\"), encoding=\"latin1\").item()\n",
    "\n",
    "    # Initialize model\n",
    "    model = Oxford17Net(x, num_classes, keep_prob, net_data)\n",
    "\n",
    "    # Link variable to model output\n",
    "    score = model.fc8\n",
    "\n",
    "    # Cross-entropy loss\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = score, labels = y))\n",
    "\n",
    "    # Accuracy of the model\n",
    "    correct_pred = tf.equal(tf.argmax(score, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    # Now test our model\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        # initialize the variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        saver.restore(sess, './checkpoints/model' \\\n",
    "                      + '_split_' + str(split_num) + '.ckpt')\n",
    "\n",
    "        # initialize the queue threads to start to shovel data\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "        # Get the model accuracy\n",
    "        a =  sess.run(accuracy)\n",
    "        print(\"Accuracy: %f\" % a)\n",
    "\n",
    "        # stop our queue threads and properly close the session\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "        sess.close()\n",
    "        \n",
    "        # return the model accuracy\n",
    "        return a\n",
    "\n",
    "# Write our results to a txt file\n",
    "f= open(\"results.txt\",\"a+\")\n",
    "\n",
    "# Store accuracies from each split of the data\n",
    "accuracy = []\n",
    "\n",
    "# Loop through the three splits\n",
    "for split in xrange(3):\n",
    "    \n",
    "    f.write(\"-\"*7)\n",
    "    f.write(\"\\nSplit \" + str(split+1) + \"\\n\")\n",
    "    f.write(\"-\"*7)\n",
    "    \n",
    "    # Reset the Tensorflow graph between training and testing\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # Train on a split\n",
    "    train(split+1)\n",
    "    \n",
    "    # Reset the Tensorflow graph between training and testing\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # Append the accuracy on this split so that we can compute mean accuracy\n",
    "    a = test(split+1)\n",
    "    f.write(\"\\nTest Accuracy: %s\" % (a) + '\\n\\n\\n')\n",
    "    accuracy.append(a)\n",
    "\n",
    "# Overall mean accuracy\n",
    "f.write(\"Mean Accuracy: %s\" % str(np.mean(accuracy)))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dl-tensorflow]",
   "language": "python",
   "name": "conda-env-dl-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
